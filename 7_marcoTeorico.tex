\section{Marco Teórico}

\subsection{Validación de Modelos}
Los modelos son representaciones matemáticas de los mecanismos que rigen los fenómenos naturales \parencite{tedeschi-2006} o como una construcción matemática diseñada para estudiar un sistema del mundo real o fenómeno (Giordano et al., 1997).\\

\textcite{medina-peralta-2017} indican que la validación de un modelo en la predicción del sistema implica la comparación, por medio de algún método, de las predicciones del modelo con los valores observados del sistema real para determinar su capacidad predictiva.\\

\textcite{mayer-butler-1993}, clasifican los métodos de validación de modelos en Evaluación Subjetiva, Técnicas Visuales, Medidas de Desviación y Pruebas Estadísticas; también señalan que debido a las complejidades y tipos de datos, no existe una combinación establecida de técnicas de validación que sea aplicable en todas las áreas.\\

Halachmi et al. (2004), menciona que la validación determina si el modelo matemático es una representación exacta del sistema real, y una forma de validación es comparando los datos reales con los predichos por el sistema.
\vspace{.5cm}

Para la validación de un modelo se evalúan la exactitud y la precisión; la primera se refiere a la proximidad de las predicciones $( z )$  con los valores observados $( y )$, por ejemplo, sus diferencias $ ( d=y-z ) $ del cero y la segunda a la dispersión de los puntos $ (z, y) $; además, en presencia de exactitud la precisión se mide cuantificando la dispersión de dichos puntos respecto a una referencia, por ejemplo, la recta determinística  $ y=x $, o bien, evaluar la varianza de las diferencias $ (\sigma_{D}^{2}) $ alrededor del cero $ (\mu_{D}=0) $ \parencite{medina-peralta-2017}.
\vspace{.5cm}

En la Figura \ref{fig:EsquemaExacPreci} se ilustra la diferencia entre la exactitud y precisión de un modelo de simulación. El caso 1 es inexacto e impreciso, el caso 2 es inexacto y preciso, el caso 3 es exacto e impreciso y el caso 4 es exacto y preciso. En un modelo de predicción lo ideal es que cumpla el caso 4. 

\begin{figure}[H]
	\centering
	\includegraphics[width=180px]{img/tadeshi_casos.png}
	\caption{Esquematización de Exactitud y Precisión. Fuente: Tedeschi (2006).}
	\label{fig:EsquemaExacPreci}
\end{figure}
\FloatBarrier

De manera similar, la Figura \ref{fig:ComparmedidExacPreci} representa los conceptos ilustrados en la Figura \ref{fig:EsquemaExacPreci} en una forma numérica; el eje $X$ y el eje $Y$ representan al modelo de los valores predichos contra los observados respectivamente. El caso 1 es inexacto e impreciso, el caso 2 es inexacto y preciso, el caso 3 es exacto e impreciso y el caso 4 es exacto y preciso. La línea punteada representa la línea de $X = Y$ . En un modelo de predicción lo ideal es que cumpla el caso 4. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/tedeshi_casos_2.png}
	\caption{ Comparación de las medidas de Exactitud y Precisión. Fuente: Tedeschi (2006).}
	\label{fig:ComparmedidExacPreci}
\end{figure}
\FloatBarrier

Las estimaciones del intercepto y la pendiente son buenos indicadores de la exactitud: cuanto mas cerca
estén simultáneamente de cero y uno respectivamente mayor es la exactitud. La estimación del coeficiente de
determinación $(R^{2})$ es un buen indicador de la precisión: cuanto mayor es la $R^{2}$ mayor es la precisión \parencite{balam-2012}.
\vspace{.5cm}

Por su parte, \textcite{mayer-butler-1993} indican que la prueba $t$ paramétrica de medias y el análisis de regresión lineal de la grafica observada frente a la predicha son los métodos estadísticos generales mas útiles, sin embargo, cada método inferencial se encuentra principalmente sujeto a las dificultades para satisfacer sus supuestos.
\vspace{.5cm}



\subsection{Regresión Lineal Simple}
Una de las técnicas mas comunes en la validación de modelos es la de Regresión Lineal Simple de los observados sobre los predichos (Mayer et al., 1994; Analla, 1998; Tedeschi, 2006).
\vspace{.5cm}


El análisis de regresión lineal tiene como objetivo modelar en forma matemática el comportamiento de una variable de respuesta en función de una o mas variables independientes
(Gutiérrez y de la Vara, 2012). Por lo tanto, se ajusta el modelo  $ y_{i} = \beta_{0} + \beta_{1}z_{i} +\epsilon $ donde $ 1 \leq i \leq n$, $ y_{i}$ es el valor real observado en la i-esima unidad experimental, $ z_{i}$ es el correspondiente valor predicho por el modelo a validar, $\epsilon_{i}$ es el componente aleatorio o error, $\beta_{0}$ es la ordenada al origen y $\beta_{1}$ la pendiente \parencite{zacarias-2023}.
\vspace{.5cm}


Esta técnica se encuentra principalmente sujeta al cumplimiento de sus supuestos. Cuando los residuales son independientes, se ajustan a una distribución normal y tienen varianza común, se aplican pruebas de hipótesis estadísticas para evaluar la exactitud, intercepto cero y pendiente uno, ya sea mediante pruebas t de Student, o bien, mediante una prueba F para determinar si el intercepto y la pendiente son simultáneamente cero y uno respectivamente \parencite{balam-2012}.




\subsubsection{Validación de Modelos con la Técnica de Regresión Lineal Simple \parencite{febles-2014}}

\textcite{febles-2014}, describe la validación de modelos con la técnica de regresión lineal simple que se centra en determinar el grado de exactitud y precisión del modelo basándose en el trabajo previo de \textcite{balam-2012}.\\

Sea un modelo $Y = H(\Theta )$, con una muestra pareada $(z_{1}, y_{1}), (z_{2}, y_{2}) , \dots , (z_{n}, y_{n})$ de valores observados y predichos respectivamente por el modelo. Bajo estas condiciones, se considera el modelo de regresión lineal: $y_{i}= \beta_{0} + \beta_{1}z_{i}$, donde $ 1 \leq i \leq n$. Donde  los estimadores de mínimos cuadrados para $\beta_{0}$,  $\beta_{1}$ y el coeficiente de determinación $R^{2}$ son respectivamente: \\

\begin{center}
	$\hat{\beta_{0}} = \hat{y} - \hat{\beta_{1}} \bar{z} $, \\
\end{center}

\begin{center}
	$ \hat{\beta_{1}} = \frac{\sum_{i=1}^{n} (z_{i} - \bar{z} ) (y_{i} - \bar{y})} {  \sum_{i=1}^{n} (z_{i} - \bar{z} )^{2} }$, \\
\end{center}

\begin{center}
	$R^{2} = \frac{ \sum_{i=1}^{n} ( \hat{y}_{i} - \bar{y})^{2}} { \sum_{i=1}^{n} ( y_{i} - \bar{y})^{2}  }$  y  $ 0 \leq R^{2} \leq 1$,
\end{center}


Donde $ \bar{z}$ y $ \bar{y}$ son las medias muéstrales de los valores observados y predichos respectivamente por el modelo; la evaluación estadístico de la exactitud y precisión depende del cumplimiento de los supuestos de normalidad y de varianza constante de los errores i en el modelo. \\


Los residuales $\epsilon_{i} = y_{i} - \bar{y}_{i}$ son estimaciones de los errores $i$ en el modelo, verificando el  supuesto de normalidad en los residuales $\epsilon_{1}, \epsilon_{2} , \dots, \epsilon_{n} $, con pruebas de bondad de ajuste, como la de Kolmogorov-Smirnov y Shapiro Wilk. El supuesto de varianza constante se verifica con el grafico de dispersión entre los predichos $y_{i}$ y los residuales $\epsilon_{i}$ del modelo. Con todas las combinaciones de los supuestos de normalidad y varianza constante, la evaluación de la exactitud y precisión del modelo $Y = H(\Theta )$ se analizan en los siguientes tres casos: cuando la muestra de residuales es independiente con distribución normal y varianza constante (NVC); cuando la muestra de residuales es independiente con distribución no normal y varianza constante (NNVC) y cuando la muestra
de residuales es independiente con varianza no constante (NVD - NNVD).

\subsubsection{Evaluación de la Exactitud de un Modelo}

Cuando se cumplen los supuestos de la Regresión, la verificación de la exactitud de un modelo puede llevarse a cabo vía dos pruebas t de Student, o bien, mediante una prueba F \parencite{balam-2012}.\\

La Prueba F Conjunta que considera el intercepto y la pendiente de manera conjunta, ambos valores agrupados en un vector de dimensión dos, a diferencia del caso anterior, se compara que el vector de parámetros  sea  contra que no lo sea. Bajo la premisa de que los residuales del modelo entre los observados y predichos cumplen los supuestos de normalidad y varianza constante,la prueba de hipótesis conjunta permite obtener una región de confianza para el vector de parámetros ,en este caso,se dice que el modelo ese exacto cuando la región de confianza conjunta contiene al punto $(0,1)$. \parencite{zacarias-2023}.\\

Para la aplicación de la F conjunta se ajusta el modelo de regresión lineal a simple con los supuestos de que los residuos ajustados se distribuyan normal, son independientes y tienen varianza constante (Ayala,2024).\\

Para la prueba de exactitud mediante la prueba de F conjunta se toma en cuenta lo siguiente \parencite{montgomery-2012}: 

\begin{center}
	$S_{zz} = \sum_{i=1}^{n}z_{i}^{2} - \frac{( \sum_{i=1}^{n} z_{i})^{2}}{n} \;$ ,
	$S_{zy} = \sum_{i=1}^{n}z_{i} y_{i} - \frac{ \sum_{i=1}^{n} z_{i} \sum_{i=1}^{n} y_{i} }{n} \;$,\\
\end{center}

\begin{center}
	$SS_{T} = \sum_{i=1}^{n}y_{i}^{2} - \frac{( \sum_{i=1}^{n} y_{i})^{2}}{n} \;$, $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_{i} \; $, $\bar{z} = \frac{1}{n} \sum_{i=1}^{n} z_{i} $ \\
\end{center}


\begin{center}
	$ \hat{\beta_{1}} = \frac{S_{zy}}{S_{zz}}  \;$, $\hat{\beta_{0}} =  \bar{y} - \hat{\beta_{a}} \bar{z}  \; $, $\hat{y} = \hat{\beta_{0}}  + \hat{\beta_{1}}z $ \\
\end{center}

\begin{center}
	$ CME =  \frac{SCE}{n-2} = \frac{ \sum_{i=1}^{n} (y_{i} - \hat{y_{i}} )^{2} }{n - 2 } =  \frac{SS_{T} - \hat{\beta_{1}} S_{zy}}{n -2} $ \\
\end{center}


La Prueba F Conjunta considera un sólo procedimiento que nos permite establecer si un modelo es exacto considerando simultáneamente el intercepto \(\beta_0\) y la pendiente \(\beta_1\), es decir, se prueba si $\bigl(\begin{smallmatrix} \beta_{0} \\ \beta_{1} \end{smallmatrix}\bigr) = \bigl(\begin{smallmatrix}0  \\  1\end{smallmatrix}\bigr) $ estadísticamente. flata qui \\

\textcite{zacarias-2023}, para evaluar la exactitud de un modelo ante los escenarios NNVC, NVD y NNVD, implemento un método que consiste en estimar la distribución de la estadística F a través de muestras Bootstrap robustas propuestas en \textcite{rana-2012}, en donde dependiendo del caso que se presente se implementan los distintos tipos de remuestreos propuestos por \textcite{wu-1986} y \textcite{liu-1988} y a partir de estas construir una región de confianza para el vector de parámetros $ \mathbf{\beta} = (\beta_{0}, \beta_{1})$.\\


\subsubsection{Evaluación de la Precisión de un Modelo}

\textcite{zacarias-2023}, menciona que una parte importante del método de regresión lineal, es que estudia si dicha relación permite realizar estimaciones con una precisión aceptable; por lo que también se considera como un criterio cuantitativo relevante para evaluar la calidad de ajuste de la regresión, el coeficiente de determinación $R^{2}$, que en general se interpreta como la proporción de la variabilidad en los datos observados $y_{i}$, que es explicada por el modelo.\\


\textcite{balam-2012}, utilizó un intervalo de confianza Bootstrap para medir la precisión de un modelo, donde el esquema de remuestreo propuesto depende del cumplimiento o no del supuesto de varianza constante. Cuando se cumple dicho supuesto se emplea un intervalo de confianza Bootstrap de residuales balanceado por el método percentil con sesgo corregido acelerado para evaluar la precisión de un modelo; y cuando no se cumple el supuesto de varianza constante se emplea un intervalo de confianza Bootstrap pareado balanceado por el método percentil con sesgo corregido acelerado.\\

\subsubsection{El Coeficiente de Determinación $R^{2}$}

El coeficiente de determinación $R^{2}$ se utiliza cuando las variables de estudio son cuantitativas y están medidas en una escala de intervalo o razón, siendo una herramienta común para medir el ajuste del modelo a los datos. El valor de $R^{2}$
se interpreta frecuentemente como la proporción de la variación de la variable dependiente explicada por el modelo de regresión lineal simple \parencite{balam-2012}. Sin embargo, es importante señalar que $R^{2}$
presenta varias limitaciones y características que deben ser tomadas en cuenta al evaluar su utilidad.\\


Según lo señalado por \textcite{balam-2012}, algunos puntos clave sobre $R^{2}$:\\

\begin{enumerate} 
	\item $R^{2}$ no mide la magnitud de la pendiente de la recta de regresión ajustada, ni implica que una pendiente grande se corresponda con un valor grande de $R^{2}$. 
	
	\item Es posible obtener un valor grande de $R^{2}$ agregando términos adicionales al modelo, lo que puede no reflejar una mejora real en la calidad del modelo.
	
	\item El valor de $R^{2}$  no mide adecuadamente la relación entre las variables si no existe una relación lineal, por lo que puede ser grande aunque 
	y no tengan una relación lineal significativa. 
	
	\item Un valor elevado de $R^{2}$ no necesariamente implica que el modelo sea un buen predictor, ya que puede estar influenciado por factores no realistas en los datos.
	
	\item En algunos casos, $R^{2}$ puede ser pequeño si el intervalo de las $z$  es demasiado estrecho para detectar una relación significativa con $y$. 
	
	\item En presencia de valores atípicos o agrupamientos (clustering) en los datos, $R^{2}$ puede ser engañoso, pues un valor grande podría ser el resultado de estos factores, en lugar de un ajuste verdadero. \\
\end{enumerate}


A pesar de sus limitaciones,  $R^{2}$ tiene ciertas ventajas, como cuando su valor es cercano a cero, lo que indica que el modelo no se ajusta bien a los datos. Sin embargo, si no hay puntos repetidos, es posible construir un modelo polinómico de grado $n-1$ que ofrezca un ajuste perfecto, lo que también pone en evidencia la necesidad de interpretar  $R^{2}$ con cautela.\\

Entre otras características, se destaca que el valor esperado de  $R^{2}$ puede aumentar o disminuir dependiendo de la variabilidad de la variable independiente $z$, y que, debido a la falta de conocimiento sobre su distribución exacta,  $R^{2}$  no debe usarse para hacer predicciones directas.\\


Dadas estas limitaciones, es importante considerar estudios adicionales que profundicen en las propiedades de $R^{2}$. \textcite{Ohtani-2004}, mencionan que para medir la precisión de un modelo de regresión lineal simple, al utilizarse tradicionalmente el coeficiente de determinación  \( R^2 \) y el coeficiente de determinación ajustado, se han presentado estudios sobre las propiedades de muestras pequeñas de \( R^2 \) y \( \overline{R}^2 \) como lo son \textcite{barten-1962} que sugiere una versión modificada de \( R^2 \) para reducir su sesgo, y la propuesta de \textcite{crammer-1987} al derivar las fórmulas exactas para los dos primeros momentos de \( R^2 \) y \( \overline{R}^2 \), quien muestra que \( R^2 \) está seriamente sesgado hacia arriba en muestras pequeñas, mientras que \( \overline{R}^2 \) es más inestable que \( R^2 \) en términos de desviación estándar.\\


Por lo cual, en \textcite{Ohtani-2004} deriva las fórmulas exactas para la función de densidad, la función de distribución y el momento \(m\)-ésimo, realizando un análisis numéricos basados en las fórmulas exactas, todo esto considerando un modelo de regresión lineal donde los términos de error obedecen a una distribución \(t\) multivariada, examinando los efectos del alejamiento de la normalidad de los términos de error en las distribuciones exactas de \(R^2\) y \(\bar{R}^2\). Con intervalos de confianza de \(R^2\) y \(\bar{R}^2\), muestran que el sesgo hacia arriba de \(R^2\) se vuelve significativo y que el error estándar de \(R^2\) aumenta a medida que los grados de libertad de la distribución de error \(t\) multivariada (\(\nu_0\)) disminuyen. También se muestra que, cuando los valores de \(\nu_0\) y el coeficiente de determinación de la población (\(\Phi\)) son pequeños, los límites superiores de confianza de \(R^2\) y \(\bar{R}^2\) son muy grandes.\\

Adicionalmente, \textcite{christou-2005} explica la distribución del coeficiente de determinación muestral en el caso de regresión simple utilizando su relación con la distribución \(F\) no central. Introduce el concepto de coeficiente de determinación verdadero, el cual es útil en estudios de simulación donde la varianza del término de error es conocida, permitiendo construir relaciones con una fuerza predeterminada.\\



\subsubsection{Verificación de los Supuestos de Normalidad y Varianza constante}




\subsection{Regresión Lineal Robusta}
En un modelo de regresión lineal, los supuestos de normalidad y homocedasticidad son fundamentales para garantizar la validez de los estimadores obtenidos por mínimos cuadrados ordinarios (OLS). Sin embargo, en la práctica, estos supuestos rara vez se cumplen debido a la presencia de valores atípicos, heterocedasticidad o discrepancias en las hipótesis del modelo. En estos casos, los estimadores de regresión robusta se presentan como una alternativa confiable, ya que están diseñados para ser menos sensibles a dichas anomalías y proporcionar inferencias más confiables sobre los parámetros del modelo \parencite{zacarias-2023}.\\

\textcite{annalisa-2024}, menciona que la regresión robusta ofrece ventajas significativas frente a métodos como la regresión ponderada basada en mínimos cuadrados iterativamente reponderados (IRWLS), especialmente en contextos con patrones heterocedásticos en los residuos. No solo permite gestionar de manera óptima los valores atípicos, sino que también proporciona un enfoque más eficaz para manejar la heterocedasticidad y las desviaciones de los supuestos clásicos del modelo.\\


El objetivo principal de la regresión robusta es desarrollar métodos que produzcan estimaciones precisas y resistentes a influencias indebidas en los datos. Una de las métricas clave para evaluar la robustez de un estimador es el punto de ruptura, definido como la mínima fracción de datos atípicos necesarios para invalidar completamente el estimador. Mientras que los estimadores clásicos como OLS tienen un punto de ruptura bajo que converge a cero a medida que aumenta el tamaño de la muestra, los estimadores robustos modernos pueden alcanzar un punto de ruptura de hasta el 50\%, lo que los hace altamente resistentes ~\parencites{siegel-1982,rousseeuw-1984}. \\

Entre los estimadores robustos más avanzados se encuentra el MM-Estimador, introducido como una combinación de alta eficiencia y robustez.El MM-Estimador es especialmente útil en contextos donde los datos contienen una proporción significativa de valores atípicos, ya que logra una eficiencia asintótica superior al 85\% y un punto de ruptura máximo del 50\% \parencite{rousseeuw-yohai-1984}. Este balance lo convierte en una herramienta clave para análisis estadísticos robustos, asegurando un ajuste confiable incluso en condiciones adversas.\\



\subsubsection{MM-Estimador \parencite{yohai-1987}}
El estimador MM tiene las siguientes propiedades: (i) es altamente eficiente cuando los errores tienen una distribución normal y (ii) su punto de ruptura es 0.5.\\

El estimador MM se define en un procedimiento de tres etapas. En la primera etapa, se calcula una estimación de regresión inicial que es consistente, robusta y con un alto punto de ruptura, pero no necesariamente eficiente. En la segunda etapa, se calcula un estimador M de la escala de errores utilizando residuos basados en la estimación inicial. Finalmente, en la tercera etapa se calcula un estimador M de los parámetros de regresión basada en una función $\psi$ descendente.\\

Considerando el modelo de regresión lineal simple:
\begin{center}
$y_{i}= \beta_{0} + \beta_{1}z_{i}$, $1 \leq i \leq n, $
\end{center}

\textcite{huber-1981} define los estimadores M de la siguiente manera: Sea una función real que satisfaga los siguientes supuestos (A1):

\begin{enumerate}[label=\roman*.]
	\item $\rho(0) = 0$.
	\item   $\rho(-u) =  \rho(u)$.
	\item $1 \leq i \leq v$ implica $\rho(u) \leq  \rho(v)$.
	\item $\rho$ es continua.
	\item Sea a = sup $\rho(u)$, entonces $0<a< \infty$.
	\item Si $\rho(u) < a $ y $0 \leq  u < v $, entonces  $\rho(u) <  \rho(v)$.
\end{enumerate}

Dada una muestra de tamaño $n$, $ \mathbf{u} = (u_{1}, u_{2}, \dots , u_{n})$,  el estimador M, $s(\mathbf{u})$ está de nido como el valor de s que es la solución de\\

\begin{center}
	$\frac{1}{n} \sum_{i=1}^{n} \rho( \frac{u_{i}}{s}) = b$,\\
\end{center}

donde $b$ puede de definirse como $E_{\Phi}(\rho(u)) = b$, donde $\Phi$ denota la distribución normal estándar.\\

Se cumple que si $c(\mathbf{u}) = \#\{i : 1 \leq i \leq n, u_{i}=0\} / n < 1- (b/a) $, entonces la sumatoria previa tiene solución única y esta solución es diferente de 0. Si $c(\mathbf{u}) \geq 1 -  (b/a)$, se define $s(\mathbf{u}) = 0$. \\

Luego, el estimador MM se define en tres etapas de la siguiente manera:\\

\begin{enumerate}
	\item Sean $ \beta_{0}^{'}$ y $ \beta_{1}^{'}$ estimaciones de los parámetros$ \beta_{0}$ y $ \beta_{1}$ respectivamente con un alto punto de ruptura, posiblemente 0.5.
	\item  Calcular los residuales\\
	\begin{center}
		 $ \epsilon_{i} = y_{i} -\beta_{0}^{'} -\beta_{1}^{'} z_{i}  $, $1 \leq i \leq n $.\\
	\end{center}
	
	y calcular $s_{n} = s(\epsilon_{i})$, el estimador M definido previamente, usando una función $\rho_{0}$ que satisface los supuestos (A1) y considerando una constante $b$ tal que $b/a = 0.5$, donde$ a = $ máx $ \rho_{0}(u)$, lo cual implica que para esta escala la estimación tiene un
	punto de ruptura igual a 0.5.
	
	\item  Sea $\rho_{1}$ otra función que satisfaga los supuestos (A1) y tal que\\
	\begin{center}
		$\rho_{1}(u) \leq \rho_{0}(u)$\\
		sup $\rho_{1}(u)$  = sup $ \rho_{0}(u) = a$\\
	\end{center}

	
	Sea $\psi_{1} = \rho_{1}^{'}$. Entonces el estimador MM, se define como cualquier solución de\\
	\begin{center}
		$\sum_{i=1}^{n} \psi_{1} (\frac{\epsilon_{i}}{s_{n}}) z_{i} = 0$\\
	\end{center}

\end{enumerate}


\subsubsection{ Algoritmo MM-Estimador \parencite{zacarias-2023} }
Como se presenta en la tesis de \textcite{zacarias-2023}, el algoritmo MM-Estimador es un enfoque robusto que mejora la estimación de los coeficientes en presencia de valores atípicos. A continuación, se describe el algoritmo:\\

Sea\\
\begin{center}
 $ y_{i} = \beta_{0} -\beta_{1}z_{i} + \epsilon_{i} $, $1 \leq i \leq n $,\\
\end{center}

una muestra de tamaño $n$ y suponga dadas las estimaciones iniciales $\beta_{0}^{'}$ y $\beta_{0}^{'}$, además del estimador M definido en la Etapa 2 como $s_{n}$. Para cada $t \: \in  \: \mathbb{R} $ se definen los pesos
$w_{i}(t) = \psi_{1} (\epsilon_{i} /s_{n}) /(\epsilon_{i} /s_{n})$. También se definen \\


\begin{center}
	$g(t) = \frac{1}{s_{n}^{2}} \sum_{i=1}^{n} w_{i}( t )\epsilon_{i} z_{i} =  \frac{1}{s_{n}}  \sum_{i=1}^{n} \psi_{1} (\frac{\epsilon_{i}}{s_{n}}) z_{i} $, y \\	
\end{center}

\begin{center}
	$ M(t) = \frac{1}{s_{n}^{2}}  \sum_{i=1}^{n}  w_{i}( t ) z_{i}^{2} $.\\
\end{center}





Si $t^{(i)}$ es el valor de la estimación en la j-esima iteración, entonces $t^{(j+1)}$ está definido por $t^{(j+1)} = t^{(j)} + \Delta ( t^{(j)})$, donde,\\

\begin{center}
	$\Delta ( t) = M^{-1}(t)g(t)$.\\
\end{center}

Sea $0< \delta  < 1$  y $-g(t)$ el gradiente de $S(t)$, donde \\

\begin{center}
	$S(t) = \sum_{i=1}^{n} \rho_{1} ( \frac{\epsilon_{i}}{s_{n}})$. \\
\end{center}


Es posible encontrar un entero $k$ \parencite{yohai-1987} tal que\\

\begin{center}
	$ S(t^{(j)} + \Delta(t^{(j)} )/2^{k}) \leq S(t^{(j)}) - \delta( \Delta(t^{(j)} )/2^{k} ) g(t^{(j)})$. \\
\end{center}


Sea $k_{1,j}$ el mínimo de dichas $k$ y sea $k_{2,j}$ el valor de $k, 0 \leq k  \leq k_{1,j}$, lo que da el mínimo de $S(t^{(j)} + \Delta(t^{(j)} )/2^{k})$. Entonces se define el paso recursivo mediante\\

\begin{center}
	$ t^{(t+1)} = (t^{j} + (1/2^{k_{2,j}}) \Delta(t^{(j)} )$.\\
\end{center}

comenzando $t_{(0)} $ las estimaciones de los parámetros de regresión $\beta_{0}$ y $\beta_{1}$.\\

Los estimadores MM considerados se basan en la función $\rho$ bicuadrada dada por:


\[
\rho(u) =
\begin{cases}
	u^{2}/2 -u^{4}/2 + u^{6}/6	, & \text{si } |u| \leq 1 \\ \\
	1/6,     & \text{si } | u | > 1\\
\end{cases}
\]


que corresponde a la función $\psi$ bicuadrada

\[
\psi(u) =
\begin{cases}
	u(1- u^{2})^{2}	, & \text{si } |u| \leq 1 \\ \\
	0,     & \text{si } | u | > 1
\end{cases}
\]



\subsection{La Técnica Bootstrap}

La Técnica Bootstrap es un método computacional intensivo que permite simular la distribución de una estadística. La idea es maestrear repetidamente los datos observados, produciendo cada vez una función de distribución empírica a partir de los datos remuestreo dos \parencite{zacarias-2023}. El Bootstrap se desarrollo por primera vez para datos independientes y distribuidos de manera idéntica, pero esta suposición se puede relajar para que sea posible realizar
estimaciones de Bootstrap a partir de datos dependientes, como los residuos de regresión o los datos de series de tiempo (Givens y Hoeting, 2013).\\


El enfoque Bootstrap esta especialmente indicado en los casos en donde los datos no siguen una distribución normal, hecho que es común a la mayor parte de las medidas utilizadas habitualmente en las ciencias del comportamiento (Micceri, 1989).\\


El Bootstrap constituye una variedad de técnicas para la inferencia estadística denominadas genéricamente métodos de remuestreo entre las que se encuentran la permutación estocástica, el Jacknife y la validación cruzada (Balam, 2012). Con los métodos de remuestreo nos permiten cuantificar la incertidumbre calculando errores estándar e intervalos de confianza y realizando pruebas de significancia.Requieren menos suposiciones que los métodos tradicionales y generalmente dan respuestas precisas (Hesterberg et al. , 2003).\\


Segun Hesterberg et al., (2003) las ventajas del bootstrap son:

\begin{enumerate}
	\item  Pocos supuestos. No requiere que la muestra sea modelada con la distribución normal o que el tamaño de muestra sea grande.
	
	\item Mayor precisión. Algunos métodos Bootstrap son mas precisos en la practica que los métodos clásicos.
	
	\item Generalidades. Los métodos de remuestreo son notablemente similares para una amplia gama de estadísticos y no requieren de nuevas formulas para cada estadístico. No es necesario memorizar o buscar formulas especiales para cada procedimiento.
\end{enumerate}


\subsubsection{El Principio Bootstrap}
Sea $\theta =T(F)$ una característica de interés de una distribución $F$ desconocida. Sea $x_{1}, x_{2}, \dots, x_{n}$ aleatoria independiente e idénticamente distribuida de la distribución $F$, sea $\chi =\{ x_{1}, x_{2}, \dots, x_{n} \}$ junto de datos y sea $\hat{F}(x) = \frac{1}{n} \sum_{i=1}^{n} I _{(-\infty, x)} (x_{i})$ la distribución empírica de la muestra. Entonces un estimador de $\theta$ es $\hat{\theta} = T(\hat{F})$ (Givens y Hoeting, 2005).\\

Suponga que se desea estimar la distribución $F$ de $\hat{\theta}$ la distribución $F_{R}$ de algún estadístico de prueba $R( \chi, F)$. Por ejemplo, un estadístico de prueba es\\

\begin{center}
	$ R( \chi, F) = [T(\hat{F}) - T(F)/S(\hat{F})]$\\
\end{center}

donde $S(\hat{F})$ estima la desviación estándar de $T(\hat{F})$. La distribución de la variable aleatoria $R( \chi, F)$ puede ser intratable o completamente desconocida. Esta distribución también puede depender de la distribución desconocida de $F$. Ante esta situación, la metodología Bootstrap proporciona una aproximación a la distribución de $R( \chi, F)$ derivada de la función de distribución empírica de los datos observados de manera numérica \parencite{balam-2012}.\\

A continuación, se detallan algoritmos Bootstrap implementados por \textcite{balam-2012}:\\

\subsubsection{Algoritmo de Remuestreo Básico \parencite{balam-2012}}

Se asume una muestra de $ x_{1}, x_{2},  \dots,  x_{n}$ independiente e idénticamente distribuida.

\begin{enumerate}
	\item Se obtienen $B$ muestras de tamaño $n$ con reemplazo y con probabilidades iguales de la muestra original. La cardinalidad de este espacio muestra es $n^{n}$. Se denotan las muestras Bootstrap por $X^{*}_{1}, X^{*}_{2},  \dots, X^{*}_{B}$ donde cada $X^{*}_{i}$ es un vector de tamaño $n$.
	
	\item Se obtienen las muestras, $\hat{\theta}^{*}_{1} = T (X^{*}_{1}) , \hat{\theta}^{*}_{2} = T (X^{*}_{2}), \dots,\hat{\theta}^{*}_{B} = T (X^{*}_{B})$.
	
	\item Se usa la distribución empírica $\hat{F}_{\hat{\theta}^{*}}$ de la muestra $\hat{\theta}^{*}_{1},\hat{\theta}^{*}_{2},  \dots, \hat{\theta}^{*}_{B}$ para estimar $F_{\hat{\theta}} $.
\end{enumerate}

En la practica, se usa una $B$ grande para disminuir el error de simulación al evitar el calculo de todo el espacio muestra Bootstrap.\\

Las estimaciones para  $F_{\hat{\theta}}, \theta^{*}$ y $ \sigma_{\theta^{*}} $ están dadas respectivamente por:

\[
\hat{F}_{\hat{\theta}^{*}} \approx F_{\hat{\theta}}, 
\hspace{.5cm} \hat{\theta}^{*} = \frac{1}{B} \sum_{i=1}^{B}  \hat{\theta}^{*}_{i} \approx \theta,
\hspace{.5cm} Var(\hat{\theta}^{*}) = \frac{1}{B-1} \sum_{i=1}^{B}(\hat{\theta}^{*}_{i}-\hat{\theta}^{*})^2
\]




\subsubsection{Algoritmo de Remuestreo Balanceado \parencite{balam-2012}}
\textcite{balam-2012}, menciona que el Bootstrap Balanceado al ser una modificación a la forma del muestreo del Bootstrap básico garantiza que los datos correspondientes a cada individuo de la muestra aparezcan el mismo numero de veces, incrementando con esto la eficiencia.\\

Se asume una muestra  $ x_{1}, x_{2},  \dots,  x_{n}$ independiente e idénticamente distribuida y supongamos que se desean obtener $B$ muestras Bootstrap.

\begin{enumerate}
	\item  Considere el vector $ X=(x_{1}, x_{2},  \dots,  x_{n}) $.
	
	\item  Generar un vector $ N= (1,2,\dots,n,1,2,\dots,n,1,2,\dots,n)$ de longitud $nB$.
	
	\item Generar una permutación aleatoria $N^{*}$ del vector $N$.
	
	\item La muestra Bootstrap haciendo lo siguiente:
	
	$X^{*}_{1}$ =  Los elementos de $X$ comprendidos desde la primera hasta la posición $n$ de $N^{*}$.\linebreak
	$X^{*}_{2}$ =  Los elementos de $X$ comprendidos desde la posición $n + 1$ hasta la posición $2n$ de $N^{*}$.\\
	\vdots\\
	$X^{*}_{B}$ =  Los elementos de $X$  cuyas posiciones son las ultimas $n$ posiciones de $N^{*}$.
	
	\item  Se obtienen las muestras, $\hat{\theta}^{*}_{1} =T (X^{*}_{1}), \hat{\theta}^{*}_{2} =T (X^{*}_{2}), \dots, \hat{\theta}^{*}_{B} =T (X^{*}_{B})$.
	
	\item  Se usa la distribución empírica $\hat{F}_{\hat{\theta}^{*}}$ de la muestra $\hat{\theta}^{*}_{1},\hat{\theta}^{*}_{2}, \dots, \hat{\theta}^{*}_{B}$ para estimar $F_{\hat{\theta}}$
	
\end{enumerate}



\subsubsection{Bootstrap en Regresión Lineal}

Sea $(y_{i}, z_{i}),  1 \leq  i \leq n$; una muestra pareada entre observados y predichos, se define el modelo de regresión lineal, $ y_{i} = \beta_{0} + \beta_{1}z_{i} +\epsilon $, teniendo con Bootstrap la idea de  estimar la distribución de la estadística $R( \chi, \theta)$ que esta en función de $\theta = (\beta_{0}, \beta_{1}) $ y de $\hat{\theta}$ \parencite{zacarias-2023}. Hay dos maneras de aplicar el Bootstrap
en un modelo de regresión: aplicando Bootstrap a la muestra de residuales o aplicando Bootstrap a la muestra Pareada entre Y y Z \parencite{balam-2012}.\\

Los siguientes algoritmos son adaptaciones de los propuestos por \textcite{balam-2012}, específicamente el Bootstrap de Residuales Balanceados y el Bootstrap Pareado Balanceado. Ambas técnicas emplean el \textbf{Algoritmo de Remuestreo Balanceado} para controlar la aleatoriedad en el proceso de remuestreo, asegurando que cada observación aparezca el mismo número de veces en todas las muestras.\\


\subsubsection{Algoritmo Bootstrap de Residuales Balanceado}

Se asume que los \( \epsilon_{i} \) son independientes e idénticamente distribuidos. El algoritmo Bootstrap para generar muestras de \( R^{2} \) es el siguiente:

\begin{enumerate}
	\item Ajustar una regresión simple para el modelo \( y_{i} = \beta_{0} + \beta_{1}z_{i} + \epsilon_{i} \).
	
	\item Obtener los residuales \( \mathbf{e}_{i} = y_{i} - \hat{y}_{i} \), para \( i = 1, 2, \dots, n \).
	
	\item Aplicar el \textit{Algoritmo de Remuestreo Balanceado} para generar \( B \) muestras de residuales balanceados \( \mathbf{e}^{*}_{1}, \mathbf{e}^{*}_{2}, \dots, \mathbf{e}^{*}_{B} \), donde cada \( \mathbf{e}^{*}_{i} \) es un vector de tamaño \( n \) generado a partir de los residuales \( \mathbf{e}_{i} \).
	
	\item Generar las nuevas observaciones \( \mathbf{y}^{*}_{1}, \mathbf{y}^{*}_{2}, \dots, \mathbf{y}^{*}_{B} \), donde cada \( \mathbf{y}^{*}_{i} \) es un vector de tamaño \( n \) tal que \( \mathbf{y}^{*}_{i} = \hat{y}_{i} + \mathbf{e}^{*}_{i} \).
	
	\item Ajustar una regresión simple entre los vectores \( \mathbf{y}^{*}_{i} \) y \( z_{i} \), y calcular \( \hat{R}^{2*}_{b} \), para \( b = 1, 2, \dots, B \).
	
	\item Obtener las muestras Bootstrap:
	\[
	\hat{R}^{2*}_{1} \hspace{.5cm} \hat{R}^{2*}_{2} \hspace{.5cm} \dots \hspace{.5cm} \hat{R}^{2*}_{B}
	\]
\end{enumerate}


\subsubsection{Algoritmo Bootstrap Pareado Balanceado}

Se asume que los errores \( \epsilon_{i} \) en el modelo 
\( y_{i} = \beta_{0} + \beta_{1}z_{i} + \epsilon_{i} \), \( i = 1, 2, \dots, n \), no tienen varianza constante, lo que implica que no son idénticamente distribuidos (Givens y Hoeting, 2005; Montgomery et al., 2006).

\begin{enumerate}
	\item Considere la muestra de pares \( \mathbf{w}_{1} = (y_{1}, z_{1}), \mathbf{w}_{2} = (y_{2}, z_{2}), \dots, \mathbf{w}_{n} = (y_{n}, z_{n}) \).
	
	\item Aplicar el \textit{Algoritmo de Remuestreo Balanceado} para generar \( B \) muestras pareadas balanceadas \( \mathbf{w}^{*}_{1}, \mathbf{w}^{*}_{2}, \dots, \mathbf{w}^{*}_{B} \), donde cada \( \mathbf{w}^{*}_{i} \) es un vector de tamaño \( n \) (conjunto de pares \( (y_{i}^{*}, z_{i}^{*}) \)), generado a partir de los pares \( \mathbf{w}_{1}, \mathbf{w}_{2}, \dots, \mathbf{w}_{n} \).
	
	\item Para cada muestra \( \mathbf{w}^{*}_{i} \), donde \( i = 1, 2, \dots, B \), obtener los vectores de observados \( \mathbf{y}^{*}_{i} \) y predichos \( \mathbf{z}^{*}_{i} \), de manera que \( \mathbf{w}^{*}_{i} = (\mathbf{y}_{i}^{*}, \mathbf{z}_{i}^{*}) \); obteniendo así las secuencias \( \mathbf{y}_{1}^{*}, \mathbf{y}_{2}^{*}, \dots, \mathbf{y}_{B}^{*} \) y \( \mathbf{z}_{1}^{*}, \mathbf{z}_{2}^{*}, \dots, \mathbf{z}_{B}^{*} \).
	
	\item Ajustar una regresión simple entre los vectores \( \mathbf{y}^{*}_{i} \) y \( \mathbf{z}_{i}^{*} \), y calcular \( \hat{R}^{2*}_{b} \) para \( b = 1, 2, \dots, B \).
	
	\item Obtener las muestras Bootstrap:
	\[
	\hat{R}^{2*}_{1} \hspace{.5cm} \hat{R}^{2*}_{2} \hspace{.5cm} \dots \hspace{.5cm} \hat{R}^{2*}_{B}
	\]
\end{enumerate}



\subsubsection{Algoritmo Bootstrap Robusto Simple}
El algoritmo Bootstrap para generar muestras Bootstrap robustas para $\hat{R}^{2}$ es el siguiente:

\begin{enumerate}
	\item Obtener el MM-Estimador $\hat{B}^{MM}$ de $B$ ... y con este  obtener los ajustados $ \hat{y}^{MM}_{i} = z_{i}B^{MM},i=1,2,..., n$.
	
	\item Obtener los residuales del modelo robusto $ e^{MM}_{i} = y_{i}-\hat{y}^{MM}_{i},i = 1,2, \dots, n$.
	
	\item Remuestrea con reemplazo y con probabilidades la muestra robusta $ e^{MM}_{1},\dots, e^{MM}_{n}$ para obtener $ e^{*MM}_{1},\dots, e^{*MM}_{n}$.
	
	\item Obtener $y^{*MM}_{i} = e^{*MM}_{i} + \hat{y}^{MM}_{i},i=1,2,..., n  $.
	
	\item Ajustar una regresión simple $ y^{*MM}_{i} = \beta_{0}^{*MM}+\beta_{1}^{*MM}z_{i} + e^{*MM}_{1i}$ y obtener $\hat{R}^{2*MM}_{1}$
	
	\item Repetir los pasos 3 a 5, $(B-1)$ veces para obtener las muestras Bootstrap:
	
	\[
	\hat{R}^{2*MM}_{1} \hspace{.5cm} \hat{R}^{2*MM}_{2} \hspace{.5cm} \dots \hspace{.5cm} \hat{R}^{2*MM}_{B}
	\]
\end{enumerate}



\subsection{Wild Bootstrap}
El Wild Bootstrap es una técnica útil cuando no se cumplen los supuestos de homocedasticidad en modelos de regresión. \textcite{wu-1986} demostró que, en presencia de heterocedasticidad, las estimaciones de mínimos cuadrados son inconsistentes y asintóticamente sesgadas. Para abordar este problema, propuso un esquema Bootstrap basado en tres métodos de remuestreo de residuales, mejorando así la precisión de las estimaciones de los parámetros del modelo. Posteriormente, \textcite{liu-1988} amplió el trabajo de Wu al desarrollar dos alternativas adicionales para generar remuestras de residuales. \\


\textcite{russell-2008} en su estudió sobre el Wild Bootstrap en modelos de regresión con perturbaciones heterocedásticas, demostraron que en casos específicos, puede lograrse una inferencia perfecta. Aunque ciertas versiones carecen de corrección de asimetría, han mostrado reducciones significativas en errores de probabilidad de rechazo en pruebas Bootstrap, incluso en muestras pequeñas o medianas.\\

Estudios posteriores, como el de \textcite{rana-2012}, implementaron algunos de los esquemas de Wu y Liu; y demostraron que ofrecen un rendimiento superior en comparación con métodos tradicionales como el Bootstrap clásico, especialmente cuando existen valores atípicos. La asignación de pesos a los residuales estabiliza la varianza de las estimaciones, lo que resulta crucial para obtener inferencias más confiables.\\


Finalmente, \textcite{zacarias-2023} aplicó estos esquemas de Wild Bootstrap para evaluar la exactitud de modelos de regresión lineal en situaciones donde los supuestos de normalidad o varianza constante no se cumplen. Con una metodología inédita que utilizó los tres esquemas propuestos por \textcite{wu-1986} y las dos variantes de \textcite{liu-1988} para construir regiones de confianza robustas, adaptándose a diferentes escenarios de residuales.\\


En los apartados siguientes, se detallarán los algoritmos del Bootstrap robusto propuesto \textcite{rana-2012} con las tres diferentes maneras de remuestreo propuesto en \textcite{wu-1986} y con las dos formas
propuestas en \textcite{liu-1988}.



\subsubsection{Técnica Robusta Basada en el Esquema Wild Bootstrap}

\textbf{Algoritmo - Esquema Bootstrap de Wu 1}

\begin{enumerate}
	\item  Ajustar un modelo $y_{i} = \beta_{0} +\beta_{1}z_{i} + \epsilon_{i}$  mediante el MM-estimador de la muestra original de observaciones para obtener los parámetros robustos $\hat{B}^{MM}$ y, obtener los ajustados $\hat{y}_{i}=z_{i}\hat{\beta}_{MM}$.
	
	\item  Calcular los residuales $ \hat{e}^{MM}_{i} = y_{i}-\hat{y}_{i},i = 1,2, \dots, n$, y obtener los residuales ponderados
	\begin{center}
	\[
	\hat{e}^{WMM}_{i} =
	\begin{cases}
		e^{MM}_{i}, & \text{si } \frac{|e^{MM}_{i}|}{\sigma_{MM}} \leq c \\ \\
		\frac{c \times e^{MM}_{i}}{ | \hat{e}^{MM}_{i} | /\sigma_{MM}},     & \text{si } \frac{|e^{MM}_{i}|}{\sigma_{MM}} > c
	\end{cases}
	\]
	\end{center} 
	donde $c$ es una constante arbitraria que se elige entre 2 y 3; mientras que $\sigma_{MM}$ es la
	raíz cuadrada del cuadrado medio del error del modelo robusto $\sigma_{MM} = \sqrt{CME}$.

	\item Obtener una muestra Bootstrap $y^{*}_{i}$, tal que 
	\begin{center}
		$y^{*}_{i} =z_{i}\hat{\beta}_{MM} + \frac{t^{*}_{i}\hat{e}^{WMM}_{i}}{\sqrt{1-h_{ii}}} $,
	\end{center}
	donde $h_{ii}$ es el i-ésimo elemento de la matriz diag$(Z(Z^{T}Z)^{-1} Z^{T})$ y el valor $t^{*}_{i}$ es el
	i-ésimo elemento de una muestra aleatoria de tamaño $n$ de una $N(0,1)$.
	
	\item  Ajustar una regresion simple $ y^{*}_{1i} = \beta^{*}_{10} +\beta^{*}_{11}x_{i} + \epsilon^{*}_{1i} $ para obtener $ \hat{R}^{2*}_{1} $.
	
		\item Repetir $B - 1$ veces veces los pasos 3 y 4 para obtener las muestras:
	\[
	\hat{R}^{2*}_{1} \hspace{.5cm} \hat{R}^{2*}_{2} \hspace{.5cm} \dots \hspace{.5cm} \hat{R}^{2*}_{B}
	\]
\end{enumerate}


\textbf{Algoritmo - Esquema Bootstrap de Wu 2}

\begin{enumerate}
	\item Repetir los pasos 1 y 2 del Algoritmo Wu 1.
	
	\item  Similar al paso 3 del Algoritmo Wu 1,se obtiene una muestra Bootstrap;
	pero el valor $t^{*}_{i}$ es el i-ésimo elemento de una muestra con reemplazo con probabilidades iguales de los residuos normalizados $a_{1},a_{2}, \dots,a_{n}$, donde
	
	\begin{center}
		$a_{i} = \frac{\hat{e}_{i}-\bar{\hat{ e}}_{i}}{ \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (\hat{e}_{i}-\bar{\hat{e}})^{2} } }$ con $ \bar{\hat{e}}= \frac{1}{n} \sum_{i=1}^{n} \hat{e}_{i} $.
	\end{center}
	
	\item Repetir los pasos 4 y 5 del Algoritmo Wu 1.
\end{enumerate}



\textbf{Algoritmo - Esquema Bootstrap de Wu 3}

\begin{enumerate}
	\item Repetir los pasos 1 y 2 del Algoritmo Wu 1.
	
	\item  Similar al paso 3 del Algoritmo Wu 1,se obtiene una muestra Bootstrap;
	pero el valor $t^{*}_{i}$ es el i-ésimo elemento de una muestra con reemplazo con probabilidades iguales del vector de residuales transformado:
	
	\begin{center}
		$R_{ai} = \frac{\hat{e}^{WMM}_{i} - Mediana(\hat{e}^{WMM}_{i})}{ NMAD(\hat{e}^{WMM}_{i})  }$ ,
	\end{center}
	donde $NMAD = \frac{1}{0.6745} Mediana\{ | \hat{e}^{WMM}_{i} - Mediana(\hat{e}^{WMM}_{i}) | \}$.
	
	\item Repetir los pasos 4 y 5 del Algoritmo Wu 1.
\end{enumerate}


\textbf{Algoritmo - Esquema Bootstrap de Liu 1}

\begin{enumerate}
	\item Repetir los pasos 1 y 2 del Algoritmo Wu 1.
	
	\item  Similar al paso 3 del Algoritmo Wu 1,se obtiene una muestra Bootstrap;
	pero el valor $t^{*}_{i}$ es el i-ésimo elemento de una muestra aleatoria de tamaño $n$ de una distribución Gamma$(\alpha = 4,\beta = 1/2)$ con función de densidad $ gz(x) = [\frac{2^{4}}{3!}]x^{3}e^{-2x}I_{(x>0)}$ 

	
	\item Repetir los pasos 4 y 5 del Algoritmo Wu 1.
\end{enumerate}



\textbf{Algoritmo - Esquema Bootstrap de Liu 2}

\begin{enumerate}
	\item Repetir los pasos 1 y 2 del Algoritmo Wu 1.
	
	\item  Similar al paso 3 del Algoritmo Wu 1,se obtiene una muestra Bootstrap;
	pero el valor $t^{*}_{i}$ es el i-ésimo elemento de una muestra aleatoria de tamaño $n$ obtenida por 
	
	\begin{center}
		$t^{*}_{i} = H_{i}D_{i}- E(H_{i})E(D_{i})$,\hspace{.5cm} $i = 1,2, \dots, n$
	\end{center}
	
	donde$ H_{1},H_{2}, \dots,H_{n}$ son variables aleatorias independientes, idénticamente normalmente distribuidas con media $(1/2)( \sqrt{17/6})+ \sqrt{1/6}$ y varianza $1/2$. De igual manera,$ D_{1},D_{2}, \dots,D_{n}$ también se distribuyen normal, son independientes e idénticamente
	distribuidas con media $(1/2)( \sqrt{17/6})- \sqrt{1/6}$ y varianza  $1/2$. Tanto $ H_{i}$ y las $ D_{i}$ son independientes entre sí.
	
	\item Repetir los pasos 4 y 5 del Algoritmo Wu 1.
\end{enumerate}



\subsection{Intervalos de Confianza Bootstrap}
Las muestras Bootstrap se pueden utilizar para calcular intervalos de confianza mas aproximados. Cuando $n \rightarrow \infty$, el Bootstrap y el intervalo estándar convergen el uno al otro; en algunas situaciones se pueden hacer correcciones al sesgo. Estas correcciones pueden significativamente mejorar la exactitud inferencial de la estimación de un intervalo \parencite{efron-tibs-1993}.\\


\textcite{good-2005}, determina que el Bootstrap puede ayudarnos a obtener una estimación del intervalo para cualquier aspecto de la distribución si las observaciones son independientes y todos provienen de una distribución con el mismo valor del parámetro que se estima.El Bootstrap es particularmente valioso cuando se trata de obtener una estimación del intervalo para una proporción o para la media y la varianza de una distribución no simétrica.\\


Según \textcite{balam-2012}, desafortunadamente, tales intervalos tienen las siguientes de ciencias:\\

\begin{enumerate}
\item Son sesgados, esto es, es mas probable que contengan ciertos valores falsos del parámetro que se estiman que el verdadero.

\item Son mas anchos y menos eficientes de lo que podrán ser.
\end{enumerate}

Motivos por los que en \textcite{balam-2012} se implementa dos métodos para corregir estas de ciencias, el primero es el método Percentil con sesgo corregido y el segundo es el Bootstrap con sesgo corregido acelerado BCa.\\

Se recomienda que el número de remuestras Bootstrap este entre 2000 y 4000 en orden para la construcción de intervalos de confianza, especialmente para los percentiles mas bajos. Meeker y Escobar (1998).\\

A continuación se presentan los algoritmos para al construcción de intervalos de confianza Bootstrap con el método Percentil de \textcite{efron-1982} y Bootstrap con sesgo corregido acelerado BCa implementado en \textcite{balam-2012}.\\

\subsubsection{Algoritmo Intervalo de Confianza Bootstrap Método Percentil}

Para calcular un intervalo de confianza Bootstrap Percentil de $\hat{\theta}$ a partir de un muestra $x_{1}, x_{2}, \dots, x_{n}$ con probabilidad $(1-\alpha)$\% y con $B$ muestras.


\begin{enumerate}
\item Se obtienen $B$ muestras de tamaño $n$ con reemplazo y con probabilidades iguales de la muestra original. Se denotan las muestras Bootstrap por $X^{*}_{1}, X^{*}_{2},  \dots, X^{*}_{B}$ donde cada $X^{*}_{i}$ es un vector de tamaño $n$.

\item Se obtienen las muestras, $\hat{\theta}^{*}_{1} = T (X^{*}_{1}) , \hat{\theta}^{*}_{2} = T (X^{*}_{2}), \dots,\hat{\theta}^{*}_{B} = T (X^{*}_{B})$.

\item Las $B$ muestras $\hat{\theta}^{*}_{1}, \hat{\theta}^{*}_{2},\dots, \hat{\theta}^{*}_{B} $ se ordenan de manera ascendente, tal que $\hat{\theta}^{*}_{1} \leq \hat{\theta}^{*}_{2} \leq \dots \leq \hat{\theta}^{*}_{B} $.

\item Determinar los cuantiles $LI$ y $LS$, para el nivel de confianza del $(1-\alpha)$\% en la muestra Bootstrap ordenada, con $LI = \hat{\theta}^{*}_{ ( \alpha/2 ) \: \times \: B} $ y $LS = \hat{\theta}^{*}_{ (1 - \alpha/2) \: \times \: B} $.

\item El intervalo de confianza esta dado por: $[LI, LS]$.
\end{enumerate}





\subsubsection{Algoritmo Intervalo de Confianza Bootstrap BCa}

Para calcular un intervalo de confianza Bootstrap BCa de $\hat{\theta}$ a partir de un muestra $x_{1}, x_{2}, \dots, x_{n}$ con probabilidad $(1-\alpha)$\% y con $B$ muestras.


\begin{enumerate}
 \item Obtener una estimación $\hat{\theta}$ a partir de los datos originales.
 
\item Se obtienen $B$ muestras de tamaño $n$ con reemplazo y con probabilidades iguales de la muestra original. Se denotan las muestras Bootstrap por $X^{*}_{1}, X^{*}_{2},  \dots, X^{*}_{B}$ donde cada $X^{*}_{i}$ es un vector de tamaño $n$.

\item Se obtienen las muestras, $\hat{\theta}^{*}_{1} = T (X^{*}_{1}) , \hat{\theta}^{*}_{2} = T (X^{*}_{2}), \dots,\hat{\theta}^{*}_{B} = T (X^{*}_{B})$.

\item Se determina la proporción $p$ de las  $\hat{\theta}^{*}_{i}$ que son mayores o iguales a $\hat{\theta}$.

\item  Determinar $Z_{0} = Z_{p}$ donde $Z_{p}$ es el cuantil en la distribución normal estándar tal que $P(Z > Z_{p}) = p$.

\item  Obtener la constante de aceleración $a$ dada por:
\begin{center}
 \Large $ a = \frac{\sum_{i=1}^{n}  (\hat{\theta}_{-iprom}  - \hat{\theta}_{-i})^{3} }{ 6 \; (\; \sum_{i=1}^{n}  ( \hat{\theta}_{-iprom}  - \hat{\theta}_{-i})^{2} \; )^{3/2}} $
\end{center}

 donde: $\hat{\theta}_{-i}$ es la estimación con los datos originales quitando la $i$-esima observación y {\normalsize $\hat{\theta}_{-iprom} = \frac{1}{n} \sum_{i=1}^{n}\hat{\theta}_{-i}$}
 
 \item Obtener {\large $Z_{L} = \frac{Z_{0} - Z_{\alpha/2}}{ 1- \alpha ( Z_{0} - Z_{\alpha/2})} + Z_{0}  $}   y {\large $Z_{U} = \frac{Z_{0} + Z_{\alpha/2}}{ 1- \alpha ( Z_{0} - Z_{\alpha/2})} + Z_{0}  $}  donde: $Z_{\alpha /2}$ es el cuantil en la
 distribución normal estándar tal que $P(Z > Z_{\alpha / 2}) = \frac{\alpha}{2}$.
 
 
 \item Encontrar $LI = INVCDF( \Phi(Z_{L}))$ y $LS = INVCDF( \Phi(Z_{U}))$ donde $INVCDF$ es el cuantil en la muestra bootstrap con probabilidad $ \Phi(Z_{L})$ y $ \Phi(Z_{U})$ respectivamente y $\Phi$ es la distribución acumulada de la normal estándar, siendo $P(\hat{\theta}^{*} < LI) = \Phi(Z_{L})$ y$P(\hat{\theta}^{*} < LS) = \Phi(Z_{U})$.
 
\item El intervalo de confianza esta dado por: $[LI, LS]$.
\end{enumerate}





















\subsection{Simulación de Modelos}



\subsection{Diseño factorial con tres factores de efectos fijos}
Cuando se quiere investigar la influencia de tres factores $(A, B$ y $C)$ sobre una o más variables de respuesta, y el número de niveles de prueba en cada uno de los factores es $a, b$ y $c$, respectivamente, se puede construir el arreglo factorial $a \times b \times c$, que consiste en $a \times b \times c$ tratamientos o puntos experimentales.
\vspace{.5cm}

Debido a que son tres factores, las posibles interacciones son:
\begin{itemize}
	\item Tres interacciones de primer orden ( $A \times B $,  $A \times C $,  $B \times C $)
	\item Una interacción de segundo orden ( $A \times B \times C $).
\end{itemize}

\textbf{Estructura de los datos, modelo y análisis}\\
Sea $y_{ijkl}$ la respuesta observada $l$-ésima cuando el factor $A$ se encuentra en el $i$-ésimo nivel, el factor $B$ en el $j$-ésimo nivel y el factor $C$ en el $k$-ésimo nivel, donde  $i = 1,2, \dots, a$, $j = 1,2, \dots, b$, $k = 1,2, \dots, c$  y $l = 1,2, \dots, n$. En general, los datos observados se verán como en la tabla siguiente.


\begin{figure}[H] 
	\centering 
	\includegraphics[width=0.90\linewidth]{img/factorial.png} 
	\caption{Disposición general para un diseño factorial con tres factores de efectos fijos.} 
	\label{fig:FactorialTres}
\end{figure}
\FloatBarrier

\textbf{El modelo para un diseño de tres factores es:}\\
	\begin{center}
	$y_{ijkl}=\mu + \tau_{i} + \beta_{j} + \gamma_{k} + (\tau \beta)_{ij} +(\tau \gamma)_{ik} + (\beta \gamma)_{jk} + (\tau \beta \gamma)_{ijk} + \epsilon_{ijkl} $

	$i = 1,2, \dots, a$, $j = 1,2, \dots, b$, $k = 1,2, \dots, c$  y $l = 1,2, \dots, n$
 \end{center}
 
Donde:
\begin{itemize}
	\item $\mu$ es la media general
	\item $\tau_{i}$ efecto del $i$-ésimo nivel del factor renglón $A$.
	\item $\beta_{j}$ efecto del $j$-ésimo nivel del factor columna $B$.
	\item $\gamma_{k} $ efecto del $k$-ésimo nivel del factor columna $C$.
	\item $(\tau \beta)_{ij} $ efecto de la interacción entre $\tau_{i}$  y  $\beta_{j}$ .
	\item $(\tau \gamma)_{ik}$ efecto de la interacción entre  $\tau_{i}$  y  $\gamma_{k}$ .
	\item $(\beta \gamma)_{jk}$ efecto de la interacción entre $\beta_{j}$ y  $\gamma_{k}$ .
	\item $(\tau \beta \gamma)_{ijk}$ efecto de la interacción entre $\tau_{i}$  , $\beta_{j}$ y $\gamma_{k}$ .
	\item $\epsilon_{ijkl}$ es el error aleatorio.
\end{itemize}


\textbf{Supuestos del modelo}\\
 $\epsilon_{ijkl} NI (0, \sigma^{2})$

\textbf{Hipótesis}\\
Para los factores
\begin{center}
	$H^{1}_{0} =\tau_{1} = \tau_{2} = \dots = \tau_{a} = 0$ vs $H^{1}_{1}$ : al menos una $\tau_{i} \neq 0$ , $i = 1,2, \dots, a$ \\
	$H^{2}_{0} =\beta_{1} = \beta_{2} = \dots = \beta_{b} = 0 $ vs $H^{2}_{1}$ : al menos una $\beta_{j} \neq 0$ , $j = 1,2, \dots,b$ \\
	$H^{3}_{0} =\gamma_{1} = \gamma_{2} = \dots = \gamma_{c} = 0 $ vs $H^{3}_{1}$ : al menos una $\gamma_{k} \neq 0$ , $k = 1,2, \dots,c$ \\
\end{center}

Para las interacciones 
\begin{center}
	$H^{4}_{0} : (\tau \beta)_{ij} = 0$ $ \forall i,j$ vs $H^{4}_{1}$ : al menos una $(\tau \beta)_{ij} \neq 0$ , $i = 1,2, \dots, a; j = 1,2, \dots,b$ \\
	$H^{5}_{0} : (\tau \gamma)_{ik} = 0$ $ \forall i,k$ vs $H^{5}_{1}$ : al menos una $(\tau \gamma)_{ik} \neq 0$ , $i = 1,2, \dots, a; k = 1,2, \dots,c$ \\
	$H^{6}_{0} : (\beta \gamma)_{jk} = 0$ $ \forall j,k$ vs $H^{6}_{1}$ : al menos una $(\beta \gamma)_{jk} \neq 0$ , $j = 1,2, \dots, b; k = 1,2, \dots, c$ \\
	$H^{7}_{0} : (\tau \beta \gamma)_{ijk} = 0$ $ \forall i,j,k$ vs $H^{7}_{1}$ : al menos una $(\tau \beta \gamma)_{ijk} \neq 0$ , $i = 1,2, \dots, a; j = 1,2, \dots, b; k = 1,2, \dots, c$ \\
\end{center}

\textbf{Suma de cuadrados}\\
Las fórmulas de cálculo para las sumas de cuadrados son:\\
$ SC_{total} = \sum_{i=1}^{a}  \sum_{j=1}^{b}  \sum_{k=1}^{c}  \sum_{l=1}^{n} y_{ijkl}^{2} - \frac{y_{....}^{2}}{abcn} $\\

$ SC_{A} = \sum_{i=1}^{a} \frac{y_{i...}^{2}}{bcn}  - \frac{y_{....}^{2}}{abcn} $\\

$ SC_{B} = \sum_{j=1}^{b} \frac{y_{.j..}^{2}}{acn}  - \frac{y_{....}^{2}}{abcn} $\\

$ SC_{C} = \sum_{k=1}^{c} \frac{y_{..k.}^{2}}{abn} - \frac{y_{....}^{2}}{abcn} $\\


\textbf{Las sumas de cuadrados para las interacciones son:}\\

$ SC_{AB} = \sum_{i=1}^{a} \sum_{j=1}^{b}  \frac{y_{ij..}^{2}}{cn}  - \frac{y_{....}^{2}}{abcn} - SC_{A} -SC_{B}  $\\

$ SC_{AC} = \sum_{i=1}^{a} \sum_{k=1}^{c}  \frac{y_{i.k.}^{2}}{bn}  - \frac{y_{....}^{2}}{abcn} - SC_{A} -SC_{C}  $\\

$ SC_{BC} = \sum_{j=1}^{b} \sum_{k=1}^{c}  \frac{y_{.jk.}^{2}}{an}  - \frac{y_{....}^{2}}{abcn} - SC_{C} -SC_{B}  $\\


$ SC_{ABC} =  \sum_{i=1}^{a}  \sum_{j=1}^{b}  \sum_{k=1}^{c}  \frac{y_{ijk.}^{2}}{n}  - \frac{y_{....}^{2}}{abcn} - SC_{A} -SC_{B} - SC_{C} -SC_{AB} -SC_{AC} -SC_{BC}  $\\


La suma de cuadrados del error puede encontrarse restando la suma de cuadrados de cada efecto principal e interacción de la suma de cuadrados total:\\

\begin{center}
	$ SC_{E} = SC_{T} -SC_{A} - SC_{B}-SC_{C} -SC_{AB} -SC_{AC} -SC_{BC} -SC_{ABC} $\\
\end{center}

O bien,
\begin{center}
	$ SC_{E} = SC_{T} - SC_{Sub(ABC)} $\\
\end{center}

Donde
\begin{center}
	$ SC_{Sub(ABC)} = \sum_{i=1}^{a}  \sum_{j=1}^{b}  \sum_{k=1}^{c} \frac{y_{ijk.}^{2}}{n} - \frac{y_{....}^{2}}{abcn}  $\\
\end{center}


\textbf{Estadísticos de prueba}\\
Para probar la significación la fuente de variabilidad $X$, se divide $CM_{X}$ por el  $CM_{E}$; de modo que los valores grandes de este cociente implican que los datos no apoyan la hipótesis nula correspondiente:
\begin{center}
	$ F^{X} = \frac{CM_{X}}{CM_{E}} \sim F_{x,abc(n-1)}  $\\
\end{center}


Donde $x$ representa los grados de libertad asociados a la fuente de variabilidad $X$.\\

\textbf{Región de rechazo}\\

Con un nivel de significación dado $\alpha$, la región de rechazo se encuentra en la cola superior de la distribución F correspondiente:

\begin{center}
	$ RR : F_{0}^{X} > F_{a;x,abc(n-1)} $\\
\end{center}

$ F_{0}^{X} $ es el valor de la estadística de prueba correspondiente.

\textbf{Valor p}\\

Se considera la fuente de variación $(X)$ con su correspondiente estadístico de prueba.

\begin{center}
	$ P_{X} =  P( F_{a;x,abc(n-1)} \geq F_{0}^{X} ) $\\
\end{center}


\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\makecell{Fuente de \\ variación} & SC & g.l & CM & $F_{0}$ & Valor - p \\ % Encabezados de columna
		\hline
		A & $SC_{A}$ & $a-1$ & $\frac{SC_{A}}{a-1}$ &  $\frac{CM_{A}}{CM_{E}}$ & $P(F \geq F_{0}^{A} )$ \\
		\hline
		B & $SC_{B}$ & $b-1$ & $\frac{SC_{B}}{b-1}$ &  $\frac{CM_{B}}{CM_{E}}$ & $P(F \geq F_{0}^{B} )$ \\
		\hline
		C & $SC_{C}$ & $c-1$ & $\frac{SC_{C}}{c-1}$ &  $\frac{CM_{C}}{CM_{E}}$ & $P(F \geq F_{0}^{C} )$ \\
		\hline
		AB & $SC_{AB}$ & $(a-1)(b-1)$ & $\frac{SC_{AB}}{(a-1)(b-1)}$ &  $\frac{CM_{AB}}{CM_{E}}$ & $P(F \geq F_{0}^{AB} )$ \\
		\hline
		AC & $SC_{AC}$ & $(a-1)(c-1)$ & $\frac{SC_{AC}}{(a-1)(c-1)}$ &  $\frac{CM_{AC}}{CM_{E}}$ & $P(F \geq F_{0}^{AC} )$ \\
		\hline
		BC & $SC_{BC}$ & $(b-1)(c-1) $ & $\frac{SC_{BC}}{(b-1)(c-1)}$ &  $\frac{CM_{BC}}{CM_{E}}$ & $P(F \geq F_{0}^{BC} )$ \\
		\hline
		ABC & $SC_{ABC}$ & $(a-1)(b-1)(c-1)$ & $\frac{SC_{ABC}}{(a-1)(b-1)(c-1)}$ &  $\frac{CM_{ABC}}{CM_{E}}$ & $P(F \geq F_{0}^{ABC} )$ \\
		\hline
		Error & $SC_{E} $ & $abc(n-1)$ & $\frac{SC_{E}}{abc(n-1)}$ & & \\
		\hline
		Total & $SC_{T}$ & $abcn-1$ & & &  \\
		\hline
	\end{tabular}
	\caption{Tabla del ANOVA para el diseño factorial de tres factores con efectos fijos.}
\end{table}
\FloatBarrier

\subsubsection{Comparación múltiple de Tukey}

Si el ANOVA indica que hay diferencia en el nivel medio de los factores resulta de interés llevar a cabo comparaciones entre las medias individuales para determinar diferencias específicas. Existiendo interacción significativa, los efectos de los factores no son independientes.\\


Con la prueba de Tukey el nivel de significación global es exactamente  cuando los tamaños de las muestras son iguales y como máximo $\alpha$ cuando los tamaños de las muestras son desiguales. Este método también puede utilizarse para construir intervalos de confianza sobre las diferencias en todos los pares de medias. Para estos intervalos, el nivel de confianza simultáneo es del $(1-\alpha)100\%$ cuando los tamaños de las muestras son iguales y de al menos $(1-\alpha)100\%$  cuando los tamaños de las muestras son desiguales.\\
%%%Revisar acentos

$H_{0}:\mu_{i} = \mu_{j}$ vs $H_{1}:\mu_{i} \neq \mu_{j}$ para toda $i \neq j$\\

 $\mu_{i} \neq \mu_{j}$ si $| \bar{Y}_{i} -\bar{Y}_{j} | > q_{\alpha} (p,f) \: \sqrt{\frac{CM_{E}}{n}} = T_{\alpha}$ (tamaños de las muestras iguales).\\
 

La tabla V del apéndice en Montgomery (2017) contiene $q_{\alpha} (p,f)$, valor del punto porcentual $\alpha$ superior del estadístico del rango estudentizado $q=\frac{ \bar{Y}_{max} -\bar{Y}_{min}}{ \sqrt{\frac{CM_{E}}{n}}}$, donde $\bar{Y}_{max}$ y $\bar{Y}_{min}$ son las medias muestrales mayor y menor respectivamente de un grupo de $p$ medias muestrales, $f$ son los gl asociados con $CM_{E}$.\\
 
Intervalo de confianza del  $(1-\alpha)100\%$ para $\mu_{i} - \mu_{j}$:\\
 
\begin{center}
	$ \bar{Y}_{max} -\bar{Y}_{min} -  q_{\alpha} (p,f) \: \sqrt{\frac{CM_{E}}{n}} \leq \mu_{i} - \mu_{j} \leq \bar{Y}_{max} -\bar{Y}_{min} + q_{\alpha} (p,f) \: \sqrt{\frac{CM_{E}}{n}} $ \\
\end{center}

Para tamaños de muestras desiguales, en la prueba de hipótesis se utiliza:\\

\begin{center}
	$ T_{\alpha} = \frac{q_{\alpha} (p,f)}{\sqrt{2}} \sqrt{CM_{E} (\frac{1}{n_{i}} + \frac{1}{n_{j}})}  $ \\
\end{center}


Los intervalos de confianza para la diferencia de los pares de medias se determinan con:\\


\begin{center}
	$ \bar{Y}_{max} -\bar{Y}_{min} -  \frac{q_{\alpha} (p,f)}{\sqrt{2}} \sqrt{CM_{E} (\frac{1}{n_{i}} + \frac{1}{n_{j}})}  \leq \mu_{i} - \mu_{j} \leq \bar{Y}_{max} -\bar{Y}_{min} + \frac{q_{\alpha} (p,f)}{\sqrt{2}} \sqrt{CM_{E} (\frac{1}{n_{i}} + \frac{1}{n_{j}})}  $ \\
\end{center}

A la versión para tamaños de las muestras diferentes se llama el procedimiento de Tukey-Kramer.
